{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac88a8ea",
   "metadata": {},
   "source": [
    "# Step 1 - Frame Dividing Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf6e90e",
   "metadata": {},
   "source": [
    "#### Install Opencv for Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa0ec21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting numpy>=1.21.2 (from opencv-python-headless)\n",
      "  Downloading numpy-2.2.5-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Using cached opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "Downloading numpy-2.2.5-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.9 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.1/12.9 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.9 MB 8.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.6/12.9 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.2/12.9 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.5/12.9 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.9 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 9.0 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, opencv-python-headless\n",
      "\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   -------------------- ------------------- 1/2 [opencv-python-headless]\n",
      "   -------------------- ------------------- 1/2 [opencv-python-headless]\n",
      "   -------------------- ------------------- 1/2 [opencv-python-headless]\n",
      "   -------------------- ------------------- 1/2 [opencv-python-headless]\n",
      "   ---------------------------------------- 2/2 [opencv-python-headless]\n",
      "\n",
      "Successfully installed numpy-2.2.5 opencv-python-headless-4.11.0.86\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55dbe94",
   "metadata": {},
   "source": [
    "#### Importing Libraries CV2 and OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd218d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af32fb",
   "metadata": {},
   "source": [
    "#### Function to Extract Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53894381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, output_folder, frame_rate=30):\n",
    "    \"\"\"Extract frames from a video at a given frame rate.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # Get video FPS\n",
    "\n",
    "    # Debug: Print FPS\n",
    "    print(f\"Video: {video_path}, FPS: {fps}\")\n",
    "\n",
    "    if fps == 0:\n",
    "        print(\"Error: Could not read FPS. Check the video file or codec.\")\n",
    "        return  # Exit function to prevent division by zero\n",
    "\n",
    "    frame_interval = max(1, int(fps / frame_rate))  # Avoid division by zero\n",
    "\n",
    "    count = 0\n",
    "    frame_id = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Stop when the video ends\n",
    "\n",
    "        if count % frame_interval == 0:\n",
    "            frame_filename = os.path.join(output_folder, f\"frame_{frame_id:04d}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            frame_id += 1  # Increment frame number\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\" Extracted {frame_id} frames from {video_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3b317",
   "metadata": {},
   "source": [
    "#### Check the orientation and Quality -Rotate if needed Automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f557962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def check_orientation(folder_path):\n",
    "    \"\"\"\n",
    "    Automatically rotates portrait images in the given folder to landscape.\n",
    "    Overwrites the original image if rotated.\n",
    "    \"\"\"\n",
    "    for img_name in sorted(os.listdir(folder_path)):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is None:\n",
    "            print(f\"‚ö†Ô∏è Skipping unreadable image: {img_name}\")\n",
    "            continue\n",
    "\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        # Rotate only if it's in portrait orientation\n",
    "        if height > width:\n",
    "            img_rotated = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "            cv2.imwrite(img_path, img_rotated)\n",
    "            print(f\"üîÑ Rotated {img_name} to landscape ({height}x{width}) ‚Üí ({img_rotated.shape[1]}x{img_rotated.shape[0]})\")\n",
    "        else:\n",
    "            print(f\"‚úÖ {img_name} already landscape ({width}x{height})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1f082",
   "metadata": {},
   "source": [
    "#### Remove Blurry Images from frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "105e241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "def detect_and_move_blurry_images(folder_path, default_threshold=120.0):\n",
    "    \"\"\"\n",
    "    Analyze blurriness of images using Laplacian variance.\n",
    "    Prompts for threshold input and moves blurry images to a separate folder.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Analyzing blurriness in: {folder_path}\")\n",
    "    blur_data = []\n",
    "\n",
    "    # Step 1: Collect and show blurriness variance\n",
    "    for img_name in sorted(os.listdir(folder_path)):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"‚ö†Ô∏è Skipping unreadable image: {img_name}\")\n",
    "            continue\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        variance = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "        blur_data.append((img_name, variance))\n",
    "\n",
    "    # Display all variance scores\n",
    "    print(\"\\nüìä Blurriness Index (Higher is sharper):\")\n",
    "    for name, var in blur_data:\n",
    "        print(f\"{name}: Variance = {var:.2f}\")\n",
    "\n",
    "    # Step 2: Ask user for threshold\n",
    "    try:\n",
    "        threshold_input = input(f\"\\nEnter blurriness threshold [Default = {default_threshold}]: \")\n",
    "        threshold = float(threshold_input) if threshold_input.strip() else default_threshold\n",
    "    except ValueError:\n",
    "        print(\"‚ö†Ô∏è Invalid input. Using default threshold.\")\n",
    "        threshold = default_threshold\n",
    "\n",
    "    # Step 3: Move blurry images\n",
    "    blurry_folder = os.path.join(os.path.dirname(folder_path), f\"blurry_{os.path.basename(folder_path)}\")\n",
    "    os.makedirs(blurry_folder, exist_ok=True)\n",
    "\n",
    "    moved = 0\n",
    "    for name, var in blur_data:\n",
    "        if var < threshold:\n",
    "            src_path = os.path.join(folder_path, name)\n",
    "            dst_path = os.path.join(blurry_folder, name)\n",
    "            shutil.move(src_path, dst_path)\n",
    "            print(f\"üìÅ Moved blurry image: {name} (Variance = {var:.2f})\")\n",
    "            moved += 1\n",
    "\n",
    "    print(f\"\\n‚úÖ Done. {moved} blurry images moved to '{blurry_folder}'.\")\n",
    "\n",
    "# Example usage\n",
    "# detect_and_move_blurry_images(\"Output/extracted_frames/top\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf70c8",
   "metadata": {},
   "source": [
    "# Step 2 - Extracting Key points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7eea05",
   "metadata": {},
   "source": [
    "#### Install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c7b3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.21-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from mediapipe) (2.2.2)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from mediapipe) (25.2.10)\n",
      "Collecting jax (from mediapipe)\n",
      "  Using cached jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Using cached jaxlib-0.6.0-cp310-cp310-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting matplotlib (from mediapipe)\n",
      "  Using cached matplotlib-3.10.1-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Collecting opencv-contrib-python (from mediapipe)\n",
      "  Using cached opencv_contrib_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from mediapipe) (4.25.7)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Using cached sounddevice-0.5.1-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from mediapipe) (0.2.0)\n",
      "Collecting CFFI>=1.0 (from sounddevice>=0.4.4->mediapipe)\n",
      "  Using cached cffi-1.17.1-cp310-cp310-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Collecting ml_dtypes>=0.5.0 (from jax->mediapipe)\n",
      "  Using cached ml_dtypes-0.5.1-cp310-cp310-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from jax->mediapipe) (1.15.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->mediapipe)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
      "Using cached mediapipe-0.10.21-cp310-cp310-win_amd64.whl (51.0 MB)\n",
      "Using cached sounddevice-0.5.1-py3-none-win_amd64.whl (363 kB)\n",
      "Using cached cffi-1.17.1-cp310-cp310-win_amd64.whl (181 kB)\n",
      "Using cached jax-0.6.0-py3-none-any.whl (2.3 MB)\n",
      "Using cached jaxlib-0.6.0-cp310-cp310-win_amd64.whl (56.4 MB)\n",
      "Using cached ml_dtypes-0.5.1-cp310-cp310-win_amd64.whl (209 kB)\n",
      "Using cached matplotlib-3.10.1-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Using cached opencv_contrib_python-4.11.0.86-cp37-abi3-win_amd64.whl (46.2 MB)\n",
      "Installing collected packages: opencv-contrib-python, ml_dtypes, contourpy, CFFI, sounddevice, matplotlib, jaxlib, jax, mediapipe\n",
      "\n",
      "   ---------------------------------------- 0/9 [opencv-contrib-python]\n",
      "   ---------------------------------------- 0/9 [opencv-contrib-python]\n",
      "   ---------------------------------------- 0/9 [opencv-contrib-python]\n",
      "   ---------------------------------------- 0/9 [opencv-contrib-python]\n",
      "   ------------- -------------------------- 3/9 [CFFI]\n",
      "   ---------------------- ----------------- 5/9 [matplotlib]\n",
      "   ---------------------- ----------------- 5/9 [matplotlib]\n",
      "   ---------------------- ----------------- 5/9 [matplotlib]\n",
      "   ---------------------- ----------------- 5/9 [matplotlib]\n",
      "   ---------------------- ----------------- 5/9 [matplotlib]\n",
      "   ---------------------- ----------------- 5/9 [matplotlib]\n",
      "   ---------------------- ----------------- 5/9 [matplotlib]\n",
      "   ---------------------- ----------------- 5/9 [matplotlib]\n",
      "   ---------------------- ----------------- 5/9 [matplotlib]\n",
      "   ---------------------- ----------------- 5/9 [matplotlib]\n",
      "   ---------------------- ----------------- 5/9 [matplotlib]\n",
      "   ---------------------- ----------------- 5/9 [matplotlib]\n",
      "   -------------------------- ------------- 6/9 [jaxlib]\n",
      "   -------------------------- ------------- 6/9 [jaxlib]\n",
      "   -------------------------- ------------- 6/9 [jaxlib]\n",
      "   -------------------------- ------------- 6/9 [jaxlib]\n",
      "   -------------------------- ------------- 6/9 [jaxlib]\n",
      "   -------------------------- ------------- 6/9 [jaxlib]\n",
      "   -------------------------- ------------- 6/9 [jaxlib]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ------------------------------- -------- 7/9 [jax]\n",
      "   ----------------------------------- ---- 8/9 [mediapipe]\n",
      "   ----------------------------------- ---- 8/9 [mediapipe]\n",
      "   ----------------------------------- ---- 8/9 [mediapipe]\n",
      "   ----------------------------------- ---- 8/9 [mediapipe]\n",
      "   ----------------------------------- ---- 8/9 [mediapipe]\n",
      "   ----------------------------------- ---- 8/9 [mediapipe]\n",
      "   ----------------------------------- ---- 8/9 [mediapipe]\n",
      "   ----------------------------------- ---- 8/9 [mediapipe]\n",
      "   ----------------------------------- ---- 8/9 [mediapipe]\n",
      "   ----------------------------------- ---- 8/9 [mediapipe]\n",
      "   ----------------------------------- ---- 8/9 [mediapipe]\n",
      "   ----------------------------------- ---- 8/9 [mediapipe]\n",
      "   ----------------------------------- ---- 8/9 [mediapipe]\n",
      "   ---------------------------------------- 9/9 [mediapipe]\n",
      "\n",
      "Successfully installed CFFI-1.17.1 contourpy-1.3.2 jax-0.6.0 jaxlib-0.6.0 matplotlib-3.10.1 mediapipe-0.10.21 ml_dtypes-0.5.1 opencv-contrib-python-4.11.0.86 sounddevice-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9cf4f",
   "metadata": {},
   "source": [
    "#### Detect Keypoints - Annotate the Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ec6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "# Initialize MediaPipe Pose model\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def detect_keypoints(input_folder, output_folder):\n",
    "    \"\"\"Detects keypoints in images and saves annotated frames.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for img_name in sorted(os.listdir(input_folder)):\n",
    "        img_path = os.path.join(input_folder, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            continue  # Skip if image is corrupted\n",
    "\n",
    "        # Convert BGR to RGB (MediaPipe expects RGB)\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(rgb_image)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Draw keypoints on the image\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Save the output image\n",
    "        output_path = os.path.join(output_folder, img_name)\n",
    "        cv2.imwrite(output_path, image)\n",
    "\n",
    "    print(f\"‚úÖ Keypoint detection completed. Output saved in {output_folder}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218aeedb",
   "metadata": {},
   "source": [
    "#### Extract 2D Key Point Data into Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71aa7083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize MediaPipe Pose model\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "def extract_2d_keypoints(input_folder, output_json):\n",
    "    \"\"\"Extracts 2D keypoints from images and saves them to a JSON file.\"\"\"\n",
    "    keypoints_data = {}\n",
    "\n",
    "    for img_name in sorted(os.listdir(input_folder)):  # Process frames in order\n",
    "        img_path = os.path.join(input_folder, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            continue  # Skip if image is corrupted\n",
    "\n",
    "        # Convert BGR to RGB (MediaPipe expects RGB)\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(rgb_image)\n",
    "\n",
    "        frame_keypoints = []\n",
    "        if results.pose_landmarks:\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                frame_keypoints.append((landmark.x, landmark.y))  # Store (x, y) only\n",
    "\n",
    "        keypoints_data[img_name] = frame_keypoints  # Store keypoints for this frame\n",
    "\n",
    "    # Save results to JSON\n",
    "    with open(output_json, 'w') as f:\n",
    "        json.dump(keypoints_data, f, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ 2D keypoints extracted and saved to {output_json}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (Extract for front, side, and top views)\n",
    "extract_2d_keypoints(\"Output Video/frames/front\", \"Output Video/keypoints/front.json\")\n",
    "extract_2d_keypoints(\"Output Video/frames/side\", \"Output Video/keypoints/side.json\")\n",
    "extract_2d_keypoints(\"Output Video/frames/top\", \"Output Video/keypoints/top.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1345a5da",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a926a61",
   "metadata": {},
   "source": [
    "- No need Because MediaPipe Already Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea923e",
   "metadata": {},
   "source": [
    "#### Pose Completness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "def check_pose_completeness_and_move(input_folder, bad_folder, min_visible=25, visibility_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Checks pose completeness and moves frames with low keypoint visibility to a separate folder.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(bad_folder):\n",
    "        os.makedirs(bad_folder)\n",
    "\n",
    "    print(f\"üîç Checking pose completeness in: {input_folder}\")\n",
    "    incomplete_frames = []\n",
    "\n",
    "    for img_name in sorted(os.listdir(input_folder)):\n",
    "        img_path = os.path.join(input_folder, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(rgb_image)\n",
    "\n",
    "        visible_count = 0\n",
    "        if results.pose_landmarks:\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                if landmark.visibility >= visibility_threshold:\n",
    "                    visible_count += 1\n",
    "\n",
    "        if visible_count < min_visible:\n",
    "            incomplete_frames.append((img_name, visible_count))\n",
    "            print(f\"‚ö†Ô∏è Incomplete pose in {img_name}: {visible_count}/33 visible\")\n",
    "\n",
    "            # Move the bad frame\n",
    "            shutil.move(img_path, os.path.join(bad_folder, img_name))\n",
    "\n",
    "    print(f\"\\nüìâ Moved {len(incomplete_frames)} incomplete frames to '{bad_folder}'\")\n",
    "    return incomplete_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move bad top-view frames\n",
    "check_pose_completeness_and_move(\n",
    "    input_folder=\"Output Video/frames/top\",\n",
    "    bad_folder=\"Output Video/frames/top_bad\"\n",
    ")\n",
    "\n",
    "# You can run it for front or side too\n",
    "# check_pose_completeness_and_move(\"Output Video/frames/front\", \"Output Video/frames/front_bad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4213c8c",
   "metadata": {},
   "source": [
    "#### Visualize Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c4745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image = plt.imread(\"Output Video/frames/front/frame_0000.jpg\")\n",
    "height, width, _ = image.shape\n",
    "\n",
    "# Convert normalized keypoints to pixel values\n",
    "x_pixels = [x * width for x, y in keypoints]\n",
    "y_pixels = [y * height for x, y in keypoints]\n",
    "\n",
    "# Display image and overlay points\n",
    "plt.imshow(image)\n",
    "plt.scatter(x_pixels, y_pixels, c=\"red\", marker=\"o\")  # Red dots for keypoints\n",
    "plt.title(\"MediaPipe Pose Annotations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da9c362",
   "metadata": {},
   "source": [
    "#### Manually Update Annoatations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_path = \"Output Video/frames/front/frame_0000.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "height, width, _ = image.shape\n",
    "\n",
    "# Replace with your actual keypoints (normalized)\n",
    "keypoints = [\n",
    "        [\n",
    "            0.5505455732345581,\n",
    "            0.5588361024856567\n",
    "        ],\n",
    "        [\n",
    "            0.5572452545166016,\n",
    "            0.552212119102478\n",
    "        ],\n",
    "        [\n",
    "            0.5614649653434753,\n",
    "            0.5519828200340271\n",
    "        ],\n",
    "        [\n",
    "            0.5650237202644348,\n",
    "            0.5518215894699097\n",
    "        ],\n",
    "        [\n",
    "            0.5457234382629395,\n",
    "            0.5524178743362427\n",
    "        ],\n",
    "        [\n",
    "            0.542575478553772,\n",
    "            0.5522381067276001\n",
    "        ],\n",
    "        [\n",
    "            0.5397322773933411,\n",
    "            0.5520192980766296\n",
    "        ],\n",
    "        [\n",
    "            0.5709152817726135,\n",
    "            0.551386833190918\n",
    "        ],\n",
    "        [\n",
    "            0.5365363359451294,\n",
    "            0.5507765412330627\n",
    "        ],\n",
    "        [\n",
    "            0.5573853850364685,\n",
    "            0.5633068680763245\n",
    "        ],\n",
    "        [\n",
    "            0.5449458360671997,\n",
    "            0.5633752346038818\n",
    "        ],\n",
    "        [\n",
    "            0.6019920110702515,\n",
    "            0.5733140110969543\n",
    "        ],\n",
    "        [\n",
    "            0.5090041756629944,\n",
    "            0.5722963213920593\n",
    "        ],\n",
    "        [\n",
    "            0.6641108989715576,\n",
    "            0.5980207920074463\n",
    "        ],\n",
    "        [\n",
    "            0.4531230330467224,\n",
    "            0.5994670391082764\n",
    "        ],\n",
    "        [\n",
    "            0.6960875988006592,\n",
    "            0.6336801648139954\n",
    "        ],\n",
    "        [\n",
    "            0.4205382168292999,\n",
    "            0.6375626921653748\n",
    "        ],\n",
    "        [\n",
    "            0.7118327617645264,\n",
    "            0.6432164907455444\n",
    "        ],\n",
    "        [\n",
    "            0.410492867231369,\n",
    "            0.6457881331443787\n",
    "        ],\n",
    "        [\n",
    "            0.7016799449920654,\n",
    "            0.6457731127738953\n",
    "        ],\n",
    "        [\n",
    "            0.42393040657043457,\n",
    "            0.6460126042366028\n",
    "        ],\n",
    "        [\n",
    "            0.6945846676826477,\n",
    "            0.6432654857635498\n",
    "        ],\n",
    "        [\n",
    "            0.42746487259864807,\n",
    "            0.6433942914009094\n",
    "        ],\n",
    "        [\n",
    "            0.5882201790809631,\n",
    "            0.6286242008209229\n",
    "        ],\n",
    "        [\n",
    "            0.5343642830848694,\n",
    "            0.6304098963737488\n",
    "        ],\n",
    "        [\n",
    "            0.6483628749847412,\n",
    "            0.6160215735435486\n",
    "        ],\n",
    "        [\n",
    "            0.46625185012817383,\n",
    "            0.6172221899032593\n",
    "        ],\n",
    "        [\n",
    "            0.6089972853660583,\n",
    "            0.656097948551178\n",
    "        ],\n",
    "        [\n",
    "            0.5100529193878174,\n",
    "            0.6553679704666138\n",
    "        ],\n",
    "        [\n",
    "            0.6025288105010986,\n",
    "            0.6603747606277466\n",
    "        ],\n",
    "        [\n",
    "            0.518900454044342,\n",
    "            0.658387303352356\n",
    "        ],\n",
    "        [\n",
    "            0.6210933923721313,\n",
    "            0.6730417609214783\n",
    "        ],\n",
    "        [\n",
    "            0.47687360644340515,\n",
    "            0.6730539202690125\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "# Convert to pixel coordinates\n",
    "keypoints_px = [(int(x * width), int(y * height)) for x, y in keypoints]\n",
    "\n",
    "# For dragging\n",
    "dragging = False\n",
    "drag_index = -1\n",
    "undo_stack = []\n",
    "\n",
    "def draw_points(img, points):\n",
    "    for i, (x, y) in enumerate(points):\n",
    "        cv2.circle(img, (x, y), 5, (0, 255, 0), -1)\n",
    "        cv2.putText(img, str(i), (x + 5, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)\n",
    "\n",
    "def get_nearest_point(x, y, points, radius=10):\n",
    "    for i, (px, py) in enumerate(points):\n",
    "        if abs(px - x) <= radius and abs(py - y) <= radius:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global dragging, drag_index, keypoints_px, undo_stack\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        idx = get_nearest_point(x, y, keypoints_px)\n",
    "        if idx != -1:\n",
    "            dragging = True\n",
    "            drag_index = idx\n",
    "            undo_stack.append(keypoints_px.copy())  # Save previous state\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if dragging and drag_index != -1:\n",
    "            keypoints_px[drag_index] = (x, y)\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        dragging = False\n",
    "        drag_index = -1\n",
    "\n",
    "cv2.namedWindow(\"Adjust Keypoints\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Adjust Keypoints\", 640, 480)\n",
    "cv2.setMouseCallback(\"Adjust Keypoints\", mouse_callback)\n",
    "\n",
    "while True:\n",
    "    temp_image = image.copy()\n",
    "    draw_points(temp_image, keypoints_px)\n",
    "    cv2.imshow(\"Adjust Keypoints\", temp_image)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    if key == ord('q'):  # Quit and save\n",
    "        break\n",
    "    elif key == ord('u'):  # Undo\n",
    "        if undo_stack:\n",
    "            keypoints_px = undo_stack.pop()\n",
    "            print(\"Undo last move.\")\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Convert to normalized keypoints\n",
    "updated_keypoints = [[x / width, y / height] for x, y in keypoints_px]\n",
    "print(\"Updated normalized keypoints:\")\n",
    "for point in updated_keypoints:\n",
    "    print(point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f69bbb6",
   "metadata": {},
   "source": [
    "#### Saving Image MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b309257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "def store_frame_metadata(image_folder, output_metadata_json):\n",
    "    \"\"\"\n",
    "    Stores metadata (filename, resolution, etc.) for annotated frames in a JSON file.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "\n",
    "    for img_name in sorted(os.listdir(image_folder)):\n",
    "        img_path = os.path.join(image_folder, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        height, width, channels = image.shape\n",
    "        metadata[img_name] = {\n",
    "            \"resolution\": f\"{width}x{height}\",\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"channels\": channels,\n",
    "            \"path\": img_path\n",
    "            # You can add more info here like timestamp if available\n",
    "        }\n",
    "\n",
    "    # Save metadata to JSON\n",
    "    with open(output_metadata_json, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "    print(f\"üìù Metadata saved to {output_metadata_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73789cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_frame_metadata(\"Output Video/keypoints/front\", \"Output Video/keypoints/front_metadata.json\")\n",
    "store_frame_metadata(\"Output Video/keypoints/side\", \"Output Video/keypoints/side_metadata.json\")\n",
    "store_frame_metadata(\"Output Video/keypoints/top\", \"Output Video/keypoints/top_metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba251890",
   "metadata": {},
   "source": [
    "# Step 3 - Feature Extraction using Media Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0cc206",
   "metadata": {},
   "source": [
    "#### Get Angles from Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fcc018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_joint_angles(input_file, output_file):\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"‚ö†Ô∏è File not found: {input_file}\")\n",
    "        return\n",
    "\n",
    "    # Detect view type from file path\n",
    "    path_lower = input_file.lower()\n",
    "    if \"front\" in path_lower:\n",
    "        view = \"front\"\n",
    "        landmarks = POSE_LANDMARKS_FRONT\n",
    "    elif \"side\" in path_lower:\n",
    "        view = \"side\"\n",
    "        landmarks = POSE_LANDMARKS_SIDE\n",
    "    elif \"top\" in path_lower:\n",
    "        view = \"top\"\n",
    "        landmarks = POSE_LANDMARKS_TOP\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Unable to detect view from path: {input_file}\")\n",
    "        return\n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "        keypoints_data = json.load(f)\n",
    "\n",
    "    angles_per_frame = {}\n",
    "\n",
    "    for frame_name, keypoints in keypoints_data.items():\n",
    "        angles = {}\n",
    "\n",
    "        def get_landmark(name):\n",
    "            index = landmarks.get(name)\n",
    "            if index is None or index >= len(keypoints):\n",
    "                print(f\"‚ö†Ô∏è Warning: Missing keypoint {name} in frame {frame_name} ({view} view)\")\n",
    "                return None\n",
    "            return keypoints[index]\n",
    "\n",
    "        if view == \"front\":\n",
    "            right_shoulder, right_elbow, right_wrist = get_landmark(\"right_shoulder\"), get_landmark(\"right_elbow\"), get_landmark(\"right_wrist\")\n",
    "            left_shoulder, left_elbow, left_wrist = get_landmark(\"left_shoulder\"), get_landmark(\"left_elbow\"), get_landmark(\"left_wrist\")\n",
    "\n",
    "            if None not in [right_shoulder, right_elbow, right_wrist]:\n",
    "                angles[\"right_elbow_angle\"] = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "            if None not in [left_shoulder, left_elbow, left_wrist]:\n",
    "                angles[\"left_elbow_angle\"] = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "\n",
    "        elif view == \"side\":\n",
    "            shoulder, elbow, wrist = get_landmark(\"shoulder\"), get_landmark(\"elbow\"), get_landmark(\"wrist\")\n",
    "            hip, knee, ankle = get_landmark(\"hip\"), get_landmark(\"knee\"), get_landmark(\"ankle\")\n",
    "            back, foot = get_landmark(\"back\"), get_landmark(\"foot\")\n",
    "\n",
    "            if None not in [hip, knee, ankle]:\n",
    "                angles[\"knee_angle\"] = calculate_angle(hip, knee, ankle)\n",
    "            if None not in [shoulder, hip, knee]:\n",
    "                angles[\"hip_angle\"] = calculate_angle(shoulder, hip, knee)\n",
    "            if None not in [knee, ankle, foot]:\n",
    "                angles[\"ankle_angle\"] = calculate_angle(knee, ankle, foot)\n",
    "            if None not in [shoulder, back, foot]:\n",
    "                angles[\"back_angle\"] = calculate_angle(shoulder, back, foot)\n",
    "            if None not in [shoulder, elbow, wrist]:\n",
    "                angles[\"elbow_angle\"] = calculate_angle(shoulder, elbow, wrist)\n",
    "\n",
    "        elif view == \"top\":\n",
    "            left_shoulder, right_shoulder = get_landmark(\"left_shoulder\"), get_landmark(\"right_shoulder\")\n",
    "            left_hip, right_hip = get_landmark(\"left_hip\"), get_landmark(\"right_hip\")\n",
    "            left_wrist, right_wrist = get_landmark(\"left_wrist\"), get_landmark(\"right_wrist\")\n",
    "\n",
    "            if None not in [left_shoulder, right_shoulder, right_hip]:\n",
    "                angles[\"shoulder_symmetry\"] = calculate_angle(left_shoulder, right_shoulder, right_hip)\n",
    "            if None not in [left_hip, right_hip, right_shoulder]:\n",
    "                angles[\"hip_symmetry\"] = calculate_angle(left_hip, right_hip, right_shoulder)\n",
    "            if None not in [left_wrist, right_wrist, right_shoulder]:\n",
    "                angles[\"wrist_alignment\"] = calculate_angle(left_wrist, right_wrist, right_shoulder)\n",
    "\n",
    "        if angles:\n",
    "            angles_per_frame[frame_name] = angles\n",
    "\n",
    "    # Save to output file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump({view: angles_per_frame}, f, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Joint angles for '{view}' view extracted and saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c498308",
   "metadata": {},
   "source": [
    "## Step 4 - Creating Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b9531",
   "metadata": {},
   "source": [
    "#### Install Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "989c2a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   ---------------------------------------- 3/3 [pandas]\n",
      "\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66fc06",
   "metadata": {},
   "source": [
    "#### Creating Dataset for GRU regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4bf7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "FEEDBACK_CATEGORIES = [\n",
    "    \"back_angle_correction\",\n",
    "    \"knee_angle_correction\",\n",
    "    \"shoulder_symmetry_correction\",\n",
    "    \"hip_symmetry_correction\",\n",
    "    \"elbow_angle_correction\"\n",
    "]\n",
    "\n",
    "def generate_feedback_continuous(avg_features):\n",
    "    return {\n",
    "        \"back_angle_correction\": 30 - avg_features[\"back_angle\"],\n",
    "        \"knee_angle_correction\": 60 - avg_features[\"knee_angle\"],\n",
    "        \"shoulder_symmetry_correction\": 180 - avg_features[\"shoulder_symmetry\"],\n",
    "        \"hip_symmetry_correction\": 180 - avg_features[\"hip_symmetry\"],\n",
    "        \"elbow_angle_correction\": 160 - avg_features[\"elbow_angle\"]\n",
    "    }\n",
    "\n",
    "def create_regression_dataset_from_json(\n",
    "    json_path: str,\n",
    "    seq_len: int = 20,\n",
    "    save_path: str = \"regression_dataset_snatch.npz\",\n",
    "    technique: str = \"snatch\",\n",
    "    metadata_log_path: str = \"regression_metadata_log.json\"\n",
    "):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        pose_data = json.load(f)\n",
    "\n",
    "    front_view = pose_data.get(\"front\", {})\n",
    "    side_view = pose_data.get(\"side\", {})\n",
    "    top_view = pose_data.get(\"top\", {})\n",
    "\n",
    "    frame_keys = set(front_view.keys()) & set(side_view.keys()) & set(top_view.keys())\n",
    "    sorted_frames = sorted(frame_keys, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "\n",
    "    X, y, t, metadata = [], [], [], []\n",
    "    skipped_count = 0\n",
    "\n",
    "    for i in range(len(sorted_frames) - seq_len + 1):\n",
    "        sequence = []\n",
    "        angle_sums = {\n",
    "            \"back_angle\": 0,\n",
    "            \"knee_angle\": 0,\n",
    "            \"shoulder_symmetry\": 0,\n",
    "            \"hip_symmetry\": 0,\n",
    "            \"elbow_angle\": 0\n",
    "        }\n",
    "\n",
    "        skip_sequence = False\n",
    "        current_frames = sorted_frames[i:i + seq_len]\n",
    "\n",
    "        for frame in current_frames:\n",
    "            try:\n",
    "                features = [\n",
    "                    front_view[frame][\"right_elbow_angle\"],\n",
    "                    front_view[frame][\"left_elbow_angle\"],\n",
    "                    side_view[frame][\"knee_angle\"],\n",
    "                    side_view[frame][\"hip_angle\"],\n",
    "                    side_view[frame][\"ankle_angle\"],\n",
    "                    side_view[frame][\"back_angle\"],\n",
    "                    side_view[frame][\"elbow_angle\"],\n",
    "                    top_view[frame][\"shoulder_symmetry\"],\n",
    "                    top_view[frame][\"hip_symmetry\"],\n",
    "                    top_view[frame][\"wrist_alignment\"]\n",
    "                ]\n",
    "\n",
    "                # Validate all angles are within 0‚Äì180\n",
    "                if any(not (0 <= val <= 180) for val in features):\n",
    "                    raise ValueError(\"Angle out of range\")\n",
    "\n",
    "                angle_sums[\"back_angle\"] += features[5]\n",
    "                angle_sums[\"knee_angle\"] += features[2]\n",
    "                angle_sums[\"shoulder_symmetry\"] += features[7]\n",
    "                angle_sums[\"hip_symmetry\"] += features[8]\n",
    "                angle_sums[\"elbow_angle\"] += features[6]\n",
    "\n",
    "                sequence.append(features)\n",
    "\n",
    "            except KeyError as ke:\n",
    "                print(f\"‚ö†Ô∏è Missing key in frame {frame}: {ke}. Skipping sequence {i}.\")\n",
    "                skip_sequence = True\n",
    "                break\n",
    "            except ValueError as ve:\n",
    "                print(f\"‚ö†Ô∏è Invalid value in frame {frame}: {ve}. Skipping sequence {i}.\")\n",
    "                skip_sequence = True\n",
    "                break\n",
    "\n",
    "        if skip_sequence:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        avg_features = {k: v / seq_len for k, v in angle_sums.items()}\n",
    "        feedback_continuous = generate_feedback_continuous(avg_features)\n",
    "        feedback_values = [feedback_continuous[cat] for cat in FEEDBACK_CATEGORIES]\n",
    "\n",
    "        X.append(sequence)\n",
    "        y.append(feedback_values)\n",
    "        t.append(technique)\n",
    "\n",
    "        # Metadata\n",
    "        metadata.append({\n",
    "            \"video_path\": json_path,\n",
    "            \"frames_used\": current_frames,\n",
    "            \"avg_features\": avg_features,\n",
    "            \"sequence_index\": i\n",
    "        })\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    t = np.array(t)\n",
    "\n",
    "    # Save dataset\n",
    "    np.savez(save_path, X=X, y=y, t=t)\n",
    "    print(f\"‚úÖ Saved regression dataset to {save_path}\")\n",
    "    print(f\"üóÇÔ∏è  Total sequences: {len(X)}, Skipped: {skipped_count}\")\n",
    "\n",
    "    # Save metadata log\n",
    "    with open(metadata_log_path, \"w\") as mf:\n",
    "        json.dump(metadata, mf, indent=2)\n",
    "        print(f\"üìù Saved metadata log to {metadata_log_path}\")\n",
    "\n",
    "    return save_path\n",
    "\n",
    "def load_and_view_regression_dataset(npz_path: str):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    X, y, t = data[\"X\"], data[\"y\"], data[\"t\"]\n",
    "\n",
    "    print(\"\\nüìä Dataset Info:\")\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    print(\"t shape:\", t.shape)\n",
    "\n",
    "    print(\"\\nüîç Sample Sequence (X[0]):\")\n",
    "    print(X[0])\n",
    "\n",
    "    print(\"\\n‚úÖ Continuous Feedback Labels (y[0]):\")\n",
    "    print(y[0])\n",
    "\n",
    "    print(\"\\nüìå Technique Label (t[0]):\")\n",
    "    print(t[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = create_regression_dataset_from_json(\n",
    "    json_path=\"joint_angles.json\",\n",
    "    seq_len=20,\n",
    "    save_path=\"regression_dataset_snatch.npz\",\n",
    "    technique=\"snatch\",\n",
    "    metadata_log_path=\"Output/datasets/regression_metadata_log.json\"\n",
    ")\n",
    "\n",
    "load_and_view_regression_dataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b878c",
   "metadata": {},
   "source": [
    "## Step 5 - Model Architecture Designing and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ab2af",
   "metadata": {},
   "source": [
    "#### Sklearn Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f2fc2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learnNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   ---------------------------------------- 3/3 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.0 scikit-learn-1.6.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d365d4c8",
   "metadata": {},
   "source": [
    "#### Tenserflow install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d242152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from tensorflow) (4.25.7)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from tensorflow) (57.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from tensorflow) (4.13.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.71.0-cp310-cp310-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Using cached h5py-3.13.0-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Downloading charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.9-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Using cached optree-0.15.0-cp310-cp310-win_amd64.whl.metadata (49 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ds\\desktop\\ai model\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.19.0-cp310-cp310-win_amd64.whl (375.7 MB)\n",
      "Using cached grpcio-1.71.0-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl (105 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.13.0-cp310-cp310-win_amd64.whl (3.0 MB)\n",
      "Using cached keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Using cached wrapt-1.17.2-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Downloading namex-0.0.9-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.15.0-cp310-cp310-win_amd64.whl (297 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, wrapt, wheel, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, mdurl, MarkupSafe, markdown, idna, h5py, grpcio, google-pasta, gast, charset-normalizer, certifi, werkzeug, requests, markdown-it-py, astunparse, tensorboard, rich, keras, tensorflow\n",
      "\n",
      "   - --------------------------------------  1/27 [libclang]\n",
      "   - --------------------------------------  1/27 [libclang]\n",
      "   ---- -----------------------------------  3/27 [wheel]\n",
      "   ----- ----------------------------------  4/27 [urllib3]\n",
      "   ------------- --------------------------  9/27 [mdurl]\n",
      "   ----------------- ---------------------- 12/27 [idna]\n",
      "   ------------------- -------------------- 13/27 [h5py]\n",
      "   ------------------- -------------------- 13/27 [h5py]\n",
      "   -------------------- ------------------- 14/27 [grpcio]\n",
      "   ----------------------- ---------------- 16/27 [gast]\n",
      "   ---------------------------- ----------- 19/27 [werkzeug]\n",
      "   ----------------------------- ---------- 20/27 [requests]\n",
      "   ------------------------------- -------- 21/27 [markdown-it-py]\n",
      "   ---------------------------------- ----- 23/27 [tensorboard]\n",
      "   ---------------------------------- ----- 23/27 [tensorboard]\n",
      "   ---------------------------------- ----- 23/27 [tensorboard]\n",
      "   ---------------------------------- ----- 23/27 [tensorboard]\n",
      "   ---------------------------------- ----- 23/27 [tensorboard]\n",
      "   ----------------------------------- ---- 24/27 [rich]\n",
      "   ----------------------------------- ---- 24/27 [rich]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tensorflow]\n",
      "   ---------------------------------------- 27/27 [tensorflow]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.2 astunparse-1.6.3 certifi-2025.4.26 charset-normalizer-3.4.2 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 idna-3.10 keras-3.9.2 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 namex-0.0.9 optree-0.15.0 requests-2.32.3 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-3.1.0 urllib3-2.4.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fafd3d6",
   "metadata": {},
   "source": [
    "#### GRU Regression Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0390ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === Load the regression dataset from .npz ===\n",
    "data = np.load(\"regression_dataset_snatch.npz\")\n",
    "X = data[\"X\"]  # shape: (samples, seq_len, features)\n",
    "y = data[\"y\"]  # shape: (samples, feedback categories)\n",
    "t = data[\"t\"]  # shape: (samples,)\n",
    "\n",
    "# === Train-validation split ===\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Define GRU regression model ===\n",
    "def build_gru_regression_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(64, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(64, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='linear'))  # Linear output for regression\n",
    "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "# === Build model ===\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (seq_len, features)\n",
    "model = build_gru_regression_model(input_shape)\n",
    "\n",
    "# === Train model ===\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# === Save model ===\n",
    "model.save(\"gru_posture_model_snatch_regression.h5\")\n",
    "\n",
    "# === Print training history ===\n",
    "print(\"‚úÖ Training complete!\")\n",
    "print(history.history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f749a590",
   "metadata": {},
   "source": [
    "## Automatic Pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190de439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# ======================= #\n",
    "# ==== CONFIGURATION ==== #\n",
    "# ======================= #\n",
    "\n",
    "# Hardcoded output paths for consistency\n",
    "FRAMES_ROOT = \"Output/frames\"\n",
    "KEYPOINTS_ROOT = \"Output/keypoints\"\n",
    "ANNOTATIONS_ROOT = \"Output/annotations\"\n",
    "LOG_FILE = \"Output/processing_log.txt\"\n",
    "CENTRAL_INDEX_FILE = \"Output/central_index.csv\"\n",
    "\n",
    "\n",
    "# Features to extract from the video\n",
    "\n",
    "ANGLES_ROOT = \"Output/angles\"\n",
    "\n",
    "# Create all required output folders at startup\n",
    "for path in [FRAMES_ROOT, KEYPOINTS_ROOT, ANNOTATIONS_ROOT, ANGLES_ROOT, os.path.dirname(LOG_FILE)]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# =============================== #\n",
    "# ==== UTILITY HELPER FUNCS ==== #\n",
    "# =============================== #\n",
    "\n",
    "def get_all_video_paths(root_folder, extensions=(\".mp4\", \".mov\")):\n",
    "    video_paths = []\n",
    "    for dirpath, _, filenames in os.walk(root_folder):\n",
    "        for file in filenames:\n",
    "            if file.lower().endswith(extensions):\n",
    "                video_paths.append(os.path.join(dirpath, file))\n",
    "    return video_paths\n",
    "\n",
    "def get_relative_path(video_path, root_folder):\n",
    "    return os.path.relpath(video_path, root_folder)\n",
    "\n",
    "def build_output_paths(video_path, root_input_folder):\n",
    "    relative_path = get_relative_path(video_path, root_input_folder)\n",
    "    base = os.path.splitext(relative_path)[0]\n",
    "    return {\n",
    "        \"frames\": os.path.join(FRAMES_ROOT, base),\n",
    "        \"keypoints\": os.path.join(KEYPOINTS_ROOT, base),\n",
    "        \"annotations\": os.path.join(ANNOTATIONS_ROOT, base),\n",
    "        \"metadata\": os.path.join(KEYPOINTS_ROOT, base, \"metadata.json\"),\n",
    "        \"angles\": os.path.join(ANGLES_ROOT, base)  #features\n",
    "    }\n",
    "\n",
    "def has_been_processed(paths):\n",
    "    return os.path.exists(paths[\"metadata\"])\n",
    "\n",
    "def log_message(message):\n",
    "    timestamp = f\"{datetime.now()} - {message}\"\n",
    "    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as log_file:\n",
    "        log_file.write(timestamp + \"\\n\")\n",
    "    print(timestamp)\n",
    "\n",
    "def save_metadata(metadata_path, data):\n",
    "    os.makedirs(os.path.dirname(metadata_path), exist_ok=True)\n",
    "    with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "def update_central_index(index_file, row):\n",
    "    os.makedirs(os.path.dirname(index_file), exist_ok=True)\n",
    "    file_exists = os.path.exists(index_file)\n",
    "    with open(index_file, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=row.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "\n",
    "# ===================================== #\n",
    "# ==== MAIN VIDEO PROCESSOR FUNC  ==== #\n",
    "# ===================================== #\n",
    "\n",
    "def process_all_videos(root_input_folder):\n",
    "    videos = get_all_video_paths(root_input_folder)\n",
    "    log_message(f\"üé¨ Found {len(videos)} videos under '{root_input_folder}'\")\n",
    "\n",
    "    for video_path in videos:\n",
    "        paths = build_output_paths(video_path, root_input_folder)\n",
    "        if has_been_processed(paths):\n",
    "            log_message(f\"‚è≠Ô∏è Skipping already processed: {video_path}\")\n",
    "            continue\n",
    "\n",
    "        # Create required output folders\n",
    "        for key in [\"frames\", \"keypoints\", \"annotations\", \"angles\"]:\n",
    "            os.makedirs(paths[key], exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            # ====== MAIN PIPELINE STEPS ======\n",
    "            # extract_frames(video_path, paths[\"frames\"])  # STEP 1\n",
    "            print(f\"Video path: {video_path}\")\n",
    "\n",
    "            # detect_keypoints(paths[\"frames\"], paths[\"annotations\"])  # STEP 2\n",
    "            extract_2d_keypoints(paths[\"frames\"], os.path.join(paths[\"keypoints\"], \"keypoints.json\"))  # STEP 3\n",
    "            \n",
    "             # ====== NEW: Extract joint angles from keypoints ======\n",
    "            extract_joint_angles(os.path.join(paths[\"keypoints\"], \"keypoints.json\"), os.path.join(paths[\"angles\"], \"joint_angles.json\"))  # STEP 4\n",
    "\n",
    "\n",
    "            # ====== Save metadata ======\n",
    "            metadata = {\n",
    "                \"video\": video_path,\n",
    "                \"processed_at\": str(datetime.now()),\n",
    "                \"frames_folder\": paths[\"frames\"],\n",
    "                \"keypoints_json\": os.path.join(paths[\"keypoints\"], \"keypoints.json\")\n",
    "            }\n",
    "            save_metadata(paths[\"metadata\"], metadata)\n",
    "\n",
    "            # ====== Update tracking CSV ======\n",
    "            update_central_index(CENTRAL_INDEX_FILE, {\n",
    "                \"video_path\": video_path,\n",
    "                \"frames_folder\": paths[\"frames\"],\n",
    "                \"keypoints_folder\": paths[\"keypoints\"],\n",
    "                \"annotation_folder\": paths[\"annotations\"],\n",
    "                \"processed_at\": metadata[\"processed_at\"]\n",
    "            })\n",
    "\n",
    "            log_message(f\"‚úÖ Processed: {video_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log_message(f\"‚ùå Error processing {video_path}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb155eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-08 19:56:20.486117 - üé¨ Found 9 videos under 'Source'\n",
      "Video path: Source\\Front\\CP\\Player02_CP_Set01_A_60Kg.mp4\n",
      "‚úÖ 2D keypoints extracted and saved to Output/keypoints\\Front\\CP\\Player02_CP_Set01_A_60Kg\\keypoints.json\n",
      "‚ö†Ô∏è Missing view: Output/keypoints\\Front\\CP\\Front\\keypoints.json\n",
      "‚ö†Ô∏è Missing view: Output/keypoints\\Front\\CP\\Side\\keypoints.json\n",
      "‚ö†Ô∏è Missing view: Output/keypoints\\Front\\CP\\Top\\keypoints.json\n",
      "‚úÖ Combined joint angles saved to 'Output/angles\\Front\\CP\\Player02_CP_Set01_A_60Kg\\joint_angles.json\\joint_angles.json'\n",
      "2025-05-08 19:56:32.460863 - ‚úÖ Processed: Source\\Front\\CP\\Player02_CP_Set01_A_60Kg.mp4\n",
      "Video path: Source\\Front\\CP\\Player02_CP_Set02_A_80Kg.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocess_all_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[57], line 101\u001b[0m, in \u001b[0;36mprocess_all_videos\u001b[1;34m(root_input_folder)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# detect_keypoints(paths[\"frames\"], paths[\"annotations\"])  # STEP 2\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m \u001b[43mextract_2d_keypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mframes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeypoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeypoints.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# STEP 3\u001b[39;00m\n\u001b[0;32m    103\u001b[0m  \u001b[38;5;66;03m# ====== NEW: Extract joint angles from keypoints ======\u001b[39;00m\n\u001b[0;32m    104\u001b[0m extract_joint_angles(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mangles\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoint_angles.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# STEP 4\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m, in \u001b[0;36mextract_2d_keypoints\u001b[1;34m(input_folder, output_json)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip if image is corrupted\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Convert BGR to RGB (MediaPipe expects RGB)\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m rgb_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m results \u001b[38;5;241m=\u001b[39m pose\u001b[38;5;241m.\u001b[39mprocess(rgb_image)\n\u001b[0;32m     25\u001b[0m frame_keypoints \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "process_all_videos(\"Source\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
