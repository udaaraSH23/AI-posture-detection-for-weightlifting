{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac88a8ea",
   "metadata": {},
   "source": [
    "# Step 1 - Frame Dividing Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf6e90e",
   "metadata": {},
   "source": [
    "#### Install Opencv for Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55dbe94",
   "metadata": {},
   "source": [
    "#### Importing Libraries CV2 and OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd218d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af32fb",
   "metadata": {},
   "source": [
    "#### Function to Extract Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53894381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, output_folder, frame_rate=30):\n",
    "    \"\"\"Extract frames from a video at a given frame rate.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # Get video FPS\n",
    "\n",
    "    # Debug: Print FPS\n",
    "    print(f\"Video: {video_path}, FPS: {fps}\")\n",
    "\n",
    "    if fps == 0:\n",
    "        print(\"Error: Could not read FPS. Check the video file or codec.\")\n",
    "        return  # Exit function to prevent division by zero\n",
    "\n",
    "    frame_interval = max(1, int(fps / frame_rate))  # Avoid division by zero\n",
    "\n",
    "    count = 0\n",
    "    frame_id = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Stop when the video ends\n",
    "\n",
    "        if count % frame_interval == 0:\n",
    "            frame_filename = os.path.join(output_folder, f\"frame_{frame_id:04d}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            frame_id += 1  # Increment frame number\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\" Extracted {frame_id} frames from {video_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3b317",
   "metadata": {},
   "source": [
    "#### Check the orientation and Quality -Rotate if needed Automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f557962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def check_orientation(folder_path):\n",
    "    \"\"\"\n",
    "    Automatically rotates portrait images in the given folder to landscape.\n",
    "    Overwrites the original image if rotated.\n",
    "    \"\"\"\n",
    "    for img_name in sorted(os.listdir(folder_path)):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is None:\n",
    "            print(f\"‚ö†Ô∏è Skipping unreadable image: {img_name}\")\n",
    "            continue\n",
    "\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        # Rotate only if it's in portrait orientation\n",
    "        if height > width:\n",
    "            img_rotated = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "            cv2.imwrite(img_path, img_rotated)\n",
    "            print(f\"üîÑ Rotated {img_name} to landscape ({height}x{width}) ‚Üí ({img_rotated.shape[1]}x{img_rotated.shape[0]})\")\n",
    "        else:\n",
    "            print(f\"‚úÖ {img_name} already landscape ({width}x{height})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1f082",
   "metadata": {},
   "source": [
    "#### Remove Blurry Images from frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "def detect_and_move_blurry_images(folder_path, default_threshold=120.0):\n",
    "    \"\"\"\n",
    "    Analyze blurriness of images using Laplacian variance.\n",
    "    Prompts for threshold input and moves blurry images to a separate folder.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Analyzing blurriness in: {folder_path}\")\n",
    "    blur_data = []\n",
    "\n",
    "    # Step 1: Collect and show blurriness variance\n",
    "    for img_name in sorted(os.listdir(folder_path)):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"‚ö†Ô∏è Skipping unreadable image: {img_name}\")\n",
    "            continue\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        variance = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "        blur_data.append((img_name, variance))\n",
    "\n",
    "    # Display all variance scores\n",
    "    print(\"\\nüìä Blurriness Index (Higher is sharper):\")\n",
    "    for name, var in blur_data:\n",
    "        print(f\"{name}: Variance = {var:.2f}\")\n",
    "\n",
    "    # Step 2: Ask user for threshold\n",
    "    try:\n",
    "        threshold_input = input(f\"\\nEnter blurriness threshold [Default = {default_threshold}]: \")\n",
    "        threshold = float(threshold_input) if threshold_input.strip() else default_threshold\n",
    "    except ValueError:\n",
    "        print(\"‚ö†Ô∏è Invalid input. Using default threshold.\")\n",
    "        threshold = default_threshold\n",
    "\n",
    "    # Step 3: Move blurry images\n",
    "    blurry_folder = os.path.join(os.path.dirname(folder_path), f\"blurry_{os.path.basename(folder_path)}\")\n",
    "    os.makedirs(blurry_folder, exist_ok=True)\n",
    "\n",
    "    moved = 0\n",
    "    for name, var in blur_data:\n",
    "        if var < threshold:\n",
    "            src_path = os.path.join(folder_path, name)\n",
    "            dst_path = os.path.join(blurry_folder, name)\n",
    "            shutil.move(src_path, dst_path)\n",
    "            print(f\"üìÅ Moved blurry image: {name} (Variance = {var:.2f})\")\n",
    "            moved += 1\n",
    "\n",
    "    print(f\"\\n‚úÖ Done. {moved} blurry images moved to '{blurry_folder}'.\")\n",
    "\n",
    "# Example usage\n",
    "# detect_and_move_blurry_images(\"Output/extracted_frames/top\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf70c8",
   "metadata": {},
   "source": [
    "# Step 2 - Extracting Key points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7eea05",
   "metadata": {},
   "source": [
    "#### Install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c7b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mediapipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9cf4f",
   "metadata": {},
   "source": [
    "#### Detect Keypoints - Annotate the Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "# Initialize MediaPipe Pose model\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def detect_keypoints(input_folder, output_folder):\n",
    "    \"\"\"Detects keypoints in images and saves annotated frames.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for img_name in sorted(os.listdir(input_folder)):\n",
    "        img_path = os.path.join(input_folder, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            continue  # Skip if image is corrupted\n",
    "\n",
    "        # Convert BGR to RGB (MediaPipe expects RGB)\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(rgb_image)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Draw keypoints on the image\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Save the output image\n",
    "        output_path = os.path.join(output_folder, img_name)\n",
    "        cv2.imwrite(output_path, image)\n",
    "\n",
    "    print(f\"‚úÖ Keypoint detection completed. Output saved in {output_folder}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218aeedb",
   "metadata": {},
   "source": [
    "#### Extract 2D Key Point Data into Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa7083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize MediaPipe Pose model\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "def extract_2d_keypoints(input_folder, output_json):\n",
    "    \"\"\"Extracts 2D keypoints from images and saves them to a JSON file.\"\"\"\n",
    "    keypoints_data = {}\n",
    "\n",
    "    for img_name in sorted(os.listdir(input_folder)):  # Process frames in order\n",
    "        img_path = os.path.join(input_folder, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            continue  # Skip if image is corrupted\n",
    "\n",
    "        # Convert BGR to RGB (MediaPipe expects RGB)\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(rgb_image)\n",
    "\n",
    "        frame_keypoints = []\n",
    "        if results.pose_landmarks:\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                frame_keypoints.append((landmark.x, landmark.y))  # Store (x, y) only\n",
    "\n",
    "        keypoints_data[img_name] = frame_keypoints  # Store keypoints for this frame\n",
    "\n",
    "    # Save results to JSON\n",
    "    with open(output_json, 'w') as f:\n",
    "        json.dump(keypoints_data, f, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ 2D keypoints extracted and saved to {output_json}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1345a5da",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a926a61",
   "metadata": {},
   "source": [
    "- No need Because MediaPipe Already Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea923e",
   "metadata": {},
   "source": [
    "#### Pose Completness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "def check_pose_completeness_and_move(input_folder, bad_folder, min_visible=25, visibility_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Checks pose completeness and moves frames with low keypoint visibility to a separate folder.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(bad_folder):\n",
    "        os.makedirs(bad_folder)\n",
    "\n",
    "    print(f\"üîç Checking pose completeness in: {input_folder}\")\n",
    "    incomplete_frames = []\n",
    "\n",
    "    for img_name in sorted(os.listdir(input_folder)):\n",
    "        img_path = os.path.join(input_folder, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(rgb_image)\n",
    "\n",
    "        visible_count = 0\n",
    "        if results.pose_landmarks:\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                if landmark.visibility >= visibility_threshold:\n",
    "                    visible_count += 1\n",
    "\n",
    "        if visible_count < min_visible:\n",
    "            incomplete_frames.append((img_name, visible_count))\n",
    "            print(f\"‚ö†Ô∏è Incomplete pose in {img_name}: {visible_count}/33 visible\")\n",
    "\n",
    "            # Move the bad frame\n",
    "            shutil.move(img_path, os.path.join(bad_folder, img_name))\n",
    "\n",
    "    print(f\"\\nüìâ Moved {len(incomplete_frames)} incomplete frames to '{bad_folder}'\")\n",
    "    return incomplete_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move bad top-view frames\n",
    "check_pose_completeness_and_move(\n",
    "    input_folder=\"Output Video/frames/top\",\n",
    "    bad_folder=\"Output Video/frames/top_bad\"\n",
    ")\n",
    "\n",
    "# You can run it for front or side too\n",
    "# check_pose_completeness_and_move(\"Output Video/frames/front\", \"Output Video/frames/front_bad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4213c8c",
   "metadata": {},
   "source": [
    "#### Visualize Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c4745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image = plt.imread(\"Output Video/frames/front/frame_0000.jpg\")\n",
    "height, width, _ = image.shape\n",
    "\n",
    "# Convert normalized keypoints to pixel values\n",
    "x_pixels = [x * width for x, y in keypoints]\n",
    "y_pixels = [y * height for x, y in keypoints]\n",
    "\n",
    "# Display image and overlay points\n",
    "plt.imshow(image)\n",
    "plt.scatter(x_pixels, y_pixels, c=\"red\", marker=\"o\")  # Red dots for keypoints\n",
    "plt.title(\"MediaPipe Pose Annotations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da9c362",
   "metadata": {},
   "source": [
    "#### Manually Update Annoatations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_path = \"Output Video/frames/front/frame_0000.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "height, width, _ = image.shape\n",
    "\n",
    "# Replace with your actual keypoints (normalized)\n",
    "keypoints = [\n",
    "        [\n",
    "            0.5505455732345581,\n",
    "            0.5588361024856567\n",
    "        ],\n",
    "        [\n",
    "            0.5572452545166016,\n",
    "            0.552212119102478\n",
    "        ],\n",
    "        [\n",
    "            0.5614649653434753,\n",
    "            0.5519828200340271\n",
    "        ],\n",
    "        [\n",
    "            0.5650237202644348,\n",
    "            0.5518215894699097\n",
    "        ],\n",
    "        [\n",
    "            0.5457234382629395,\n",
    "            0.5524178743362427\n",
    "        ],\n",
    "        [\n",
    "            0.542575478553772,\n",
    "            0.5522381067276001\n",
    "        ],\n",
    "        [\n",
    "            0.5397322773933411,\n",
    "            0.5520192980766296\n",
    "        ],\n",
    "        [\n",
    "            0.5709152817726135,\n",
    "            0.551386833190918\n",
    "        ],\n",
    "        [\n",
    "            0.5365363359451294,\n",
    "            0.5507765412330627\n",
    "        ],\n",
    "        [\n",
    "            0.5573853850364685,\n",
    "            0.5633068680763245\n",
    "        ],\n",
    "        [\n",
    "            0.5449458360671997,\n",
    "            0.5633752346038818\n",
    "        ],\n",
    "        [\n",
    "            0.6019920110702515,\n",
    "            0.5733140110969543\n",
    "        ],\n",
    "        [\n",
    "            0.5090041756629944,\n",
    "            0.5722963213920593\n",
    "        ],\n",
    "        [\n",
    "            0.6641108989715576,\n",
    "            0.5980207920074463\n",
    "        ],\n",
    "        [\n",
    "            0.4531230330467224,\n",
    "            0.5994670391082764\n",
    "        ],\n",
    "        [\n",
    "            0.6960875988006592,\n",
    "            0.6336801648139954\n",
    "        ],\n",
    "        [\n",
    "            0.4205382168292999,\n",
    "            0.6375626921653748\n",
    "        ],\n",
    "        [\n",
    "            0.7118327617645264,\n",
    "            0.6432164907455444\n",
    "        ],\n",
    "        [\n",
    "            0.410492867231369,\n",
    "            0.6457881331443787\n",
    "        ],\n",
    "        [\n",
    "            0.7016799449920654,\n",
    "            0.6457731127738953\n",
    "        ],\n",
    "        [\n",
    "            0.42393040657043457,\n",
    "            0.6460126042366028\n",
    "        ],\n",
    "        [\n",
    "            0.6945846676826477,\n",
    "            0.6432654857635498\n",
    "        ],\n",
    "        [\n",
    "            0.42746487259864807,\n",
    "            0.6433942914009094\n",
    "        ],\n",
    "        [\n",
    "            0.5882201790809631,\n",
    "            0.6286242008209229\n",
    "        ],\n",
    "        [\n",
    "            0.5343642830848694,\n",
    "            0.6304098963737488\n",
    "        ],\n",
    "        [\n",
    "            0.6483628749847412,\n",
    "            0.6160215735435486\n",
    "        ],\n",
    "        [\n",
    "            0.46625185012817383,\n",
    "            0.6172221899032593\n",
    "        ],\n",
    "        [\n",
    "            0.6089972853660583,\n",
    "            0.656097948551178\n",
    "        ],\n",
    "        [\n",
    "            0.5100529193878174,\n",
    "            0.6553679704666138\n",
    "        ],\n",
    "        [\n",
    "            0.6025288105010986,\n",
    "            0.6603747606277466\n",
    "        ],\n",
    "        [\n",
    "            0.518900454044342,\n",
    "            0.658387303352356\n",
    "        ],\n",
    "        [\n",
    "            0.6210933923721313,\n",
    "            0.6730417609214783\n",
    "        ],\n",
    "        [\n",
    "            0.47687360644340515,\n",
    "            0.6730539202690125\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "# Convert to pixel coordinates\n",
    "keypoints_px = [(int(x * width), int(y * height)) for x, y in keypoints]\n",
    "\n",
    "# For dragging\n",
    "dragging = False\n",
    "drag_index = -1\n",
    "undo_stack = []\n",
    "\n",
    "def draw_points(img, points):\n",
    "    for i, (x, y) in enumerate(points):\n",
    "        cv2.circle(img, (x, y), 5, (0, 255, 0), -1)\n",
    "        cv2.putText(img, str(i), (x + 5, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)\n",
    "\n",
    "def get_nearest_point(x, y, points, radius=10):\n",
    "    for i, (px, py) in enumerate(points):\n",
    "        if abs(px - x) <= radius and abs(py - y) <= radius:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global dragging, drag_index, keypoints_px, undo_stack\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        idx = get_nearest_point(x, y, keypoints_px)\n",
    "        if idx != -1:\n",
    "            dragging = True\n",
    "            drag_index = idx\n",
    "            undo_stack.append(keypoints_px.copy())  # Save previous state\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if dragging and drag_index != -1:\n",
    "            keypoints_px[drag_index] = (x, y)\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        dragging = False\n",
    "        drag_index = -1\n",
    "\n",
    "cv2.namedWindow(\"Adjust Keypoints\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Adjust Keypoints\", 640, 480)\n",
    "cv2.setMouseCallback(\"Adjust Keypoints\", mouse_callback)\n",
    "\n",
    "while True:\n",
    "    temp_image = image.copy()\n",
    "    draw_points(temp_image, keypoints_px)\n",
    "    cv2.imshow(\"Adjust Keypoints\", temp_image)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    if key == ord('q'):  # Quit and save\n",
    "        break\n",
    "    elif key == ord('u'):  # Undo\n",
    "        if undo_stack:\n",
    "            keypoints_px = undo_stack.pop()\n",
    "            print(\"Undo last move.\")\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Convert to normalized keypoints\n",
    "updated_keypoints = [[x / width, y / height] for x, y in keypoints_px]\n",
    "print(\"Updated normalized keypoints:\")\n",
    "for point in updated_keypoints:\n",
    "    print(point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f69bbb6",
   "metadata": {},
   "source": [
    "#### Saving Image MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b309257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "def store_frame_metadata(image_folder, output_metadata_json):\n",
    "    \"\"\"\n",
    "    Stores metadata (filename, resolution, etc.) for annotated frames in a JSON file.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "\n",
    "    for img_name in sorted(os.listdir(image_folder)):\n",
    "        img_path = os.path.join(image_folder, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        height, width, channels = image.shape\n",
    "        metadata[img_name] = {\n",
    "            \"resolution\": f\"{width}x{height}\",\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"channels\": channels,\n",
    "            \"path\": img_path\n",
    "            # You can add more info here like timestamp if available\n",
    "        }\n",
    "\n",
    "    # Save metadata to JSON\n",
    "    with open(output_metadata_json, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "    print(f\"üìù Metadata saved to {output_metadata_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73789cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_frame_metadata(\"Output Video/keypoints/front\", \"Output Video/keypoints/front_metadata.json\")\n",
    "store_frame_metadata(\"Output Video/keypoints/side\", \"Output Video/keypoints/side_metadata.json\")\n",
    "store_frame_metadata(\"Output Video/keypoints/top\", \"Output Video/keypoints/top_metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba251890",
   "metadata": {},
   "source": [
    "# Step 3 - Feature Extraction using Media Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0cc206",
   "metadata": {},
   "source": [
    "#### Get Angles from Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fcc018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    ba, bc = a - b, c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
    "\n",
    "POSE_LANDMARKS = {\n",
    "    \"front\": {\n",
    "        \"right_shoulder\": 12, \"right_elbow\": 14, \"right_wrist\": 16,\n",
    "        \"left_shoulder\": 11, \"left_elbow\": 13, \"left_wrist\": 15\n",
    "    },\n",
    "    \"side\": {\n",
    "        \"shoulder\": 12, \"elbow\": 14, \"wrist\": 16,\n",
    "        \"hip\": 24, \"knee\": 26, \"ankle\": 28,\n",
    "        \"back\": 11, \"foot\": 32\n",
    "    },\n",
    "    \"top\": {\n",
    "        \"left_shoulder\": 11, \"right_shoulder\": 12,\n",
    "        \"left_hip\": 23, \"right_hip\": 24,\n",
    "        \"left_wrist\": 15, \"right_wrist\": 16\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_view_from_path(path):\n",
    "    folder_name = os.path.basename(os.path.dirname(path)).lower()\n",
    "    if \"front\" in folder_name:\n",
    "        return \"front\"\n",
    "    elif \"side\" in folder_name:\n",
    "        return \"side\"\n",
    "    elif \"top\" in folder_name:\n",
    "        return \"top\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def extract_joint_angles(input_path, output_path):\n",
    "    view = get_view_from_path(input_path)\n",
    "    if view == \"unknown\":\n",
    "        print(f\"‚ö†Ô∏è Skipping: Unknown view in path: {input_path}\")\n",
    "        return\n",
    "\n",
    "    with open(input_path, \"r\") as file:\n",
    "        keypoints_data = json.load(file)\n",
    "\n",
    "    landmarks = POSE_LANDMARKS[view]\n",
    "    angles_per_frame = {}\n",
    "\n",
    "    for frame_name, keypoints in keypoints_data.items():\n",
    "        angles = {}\n",
    "\n",
    "        def get_landmark(name):\n",
    "            index = landmarks.get(name)\n",
    "            if index is None or index >= len(keypoints):\n",
    "                return None\n",
    "            return keypoints[index]\n",
    "\n",
    "        if view == \"front\":\n",
    "            r_s, r_e, r_w = get_landmark(\"right_shoulder\"), get_landmark(\"right_elbow\"), get_landmark(\"right_wrist\")\n",
    "            l_s, l_e, l_w = get_landmark(\"left_shoulder\"), get_landmark(\"left_elbow\"), get_landmark(\"left_wrist\")\n",
    "            if None not in [r_s, r_e, r_w]:\n",
    "                angles[\"right_elbow_angle\"] = calculate_angle(r_s, r_e, r_w)\n",
    "            if None not in [l_s, l_e, l_w]:\n",
    "                angles[\"left_elbow_angle\"] = calculate_angle(l_s, l_e, l_w)\n",
    "\n",
    "        elif view == \"side\":\n",
    "            s, e, w = get_landmark(\"shoulder\"), get_landmark(\"elbow\"), get_landmark(\"wrist\")\n",
    "            h, k, a = get_landmark(\"hip\"), get_landmark(\"knee\"), get_landmark(\"ankle\")\n",
    "            b, f = get_landmark(\"back\"), get_landmark(\"foot\")\n",
    "            if None not in [h, k, a]:\n",
    "                angles[\"knee_angle\"] = calculate_angle(h, k, a)\n",
    "            if None not in [s, h, k]:\n",
    "                angles[\"hip_angle\"] = calculate_angle(s, h, k)\n",
    "            if None not in [k, a, f]:\n",
    "                angles[\"ankle_angle\"] = calculate_angle(k, a, f)\n",
    "            if None not in [s, b, f]:\n",
    "                angles[\"back_angle\"] = calculate_angle(s, b, f)\n",
    "            if None not in [s, e, w]:\n",
    "                angles[\"elbow_angle\"] = calculate_angle(s, e, w)\n",
    "\n",
    "        elif view == \"top\":\n",
    "            l_s, r_s = get_landmark(\"left_shoulder\"), get_landmark(\"right_shoulder\")\n",
    "            l_h, r_h = get_landmark(\"left_hip\"), get_landmark(\"right_hip\")\n",
    "            l_w, r_w = get_landmark(\"left_wrist\"), get_landmark(\"right_wrist\")\n",
    "            if None not in [l_s, r_s, r_h]:\n",
    "                angles[\"shoulder_symmetry\"] = calculate_angle(l_s, r_s, r_h)\n",
    "            if None not in [l_h, r_h, r_s]:\n",
    "                angles[\"hip_symmetry\"] = calculate_angle(l_h, r_h, r_s)\n",
    "            if None not in [l_w, r_w, r_s]:\n",
    "                angles[\"wrist_alignment\"] = calculate_angle(l_w, r_w, r_s)\n",
    "\n",
    "        if angles:\n",
    "            angles_per_frame[frame_name] = angles\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(angles_per_frame, f, indent=4)\n",
    "    print(f\"‚úÖ Saved joint angles to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c498308",
   "metadata": {},
   "source": [
    "# Step 4 - Creating Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b9531",
   "metadata": {},
   "source": [
    "#### Install Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66fc06",
   "metadata": {},
   "source": [
    "#### Creating Dataset for GRU regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4bf7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set of processed players to avoid duplicates\n",
    "processed_players = set()\n",
    "\n",
    "# Feedback categories mapped to model output vector\n",
    "FEEDBACK_CATEGORIES = [\n",
    "    \"back_angle_correction\",\n",
    "    \"knee_angle_correction\",\n",
    "    \"shoulder_symmetry_correction\",\n",
    "    \"hip_symmetry_correction\",\n",
    "    \"elbow_angle_correction\"\n",
    "]\n",
    "\n",
    "# Target optimal values for feedback (domain-specific)\n",
    "TARGETS = {\n",
    "    \"back_angle\": 30,\n",
    "    \"knee_angle\": 60,\n",
    "    \"shoulder_symmetry\": 180,\n",
    "    \"hip_symmetry\": 180,\n",
    "    \"elbow_angle\": 160\n",
    "}\n",
    "\n",
    "def generate_feedback_continuous(avg_features):\n",
    "    \"\"\"Generates continuous feedback values as the difference from optimal angles.\"\"\"\n",
    "    return {\n",
    "        category: TARGETS[category.replace(\"_correction\", \"\")] - avg_features[category.replace(\"_correction\", \"\")]\n",
    "        for category in FEEDBACK_CATEGORIES\n",
    "    }\n",
    "\n",
    "def create_regression_dataset(\n",
    "    folder_path: str,\n",
    "    seq_len: int = 20,\n",
    "    save_path: str = \"regression_dataset_snatch.npz\",\n",
    "    technique: str = \"snatch\",\n",
    "    metadata_log_path: str = \"regression_metadata_log.json\"\n",
    "):\n",
    "    # Extract player name from folder path\n",
    "    folder_parts = folder_path.split(os.path.sep)\n",
    "    player_name = folder_parts[-2]\n",
    "\n",
    "    if player_name in processed_players:\n",
    "        print(f\"‚è© Skipping already processed player: {player_name}\")\n",
    "        return save_path\n",
    "\n",
    "    print(f\"üîç Processing player: {player_name}...\")\n",
    "\n",
    "    # Construct view paths\n",
    "    base_dir = folder_parts[0]\n",
    "    sub_path = os.path.join(*folder_parts[2:])\n",
    "    front_path = os.path.join(base_dir, \"Front\", sub_path)\n",
    "    side_path = os.path.join(base_dir, \"Side\", sub_path)\n",
    "    top_path = os.path.join(base_dir, \"Top\", sub_path)\n",
    "\n",
    "    try:\n",
    "        with open(os.path.join(front_path, \"joint_angles.json\"), \"r\") as f:\n",
    "            front_view = json.load(f)\n",
    "        with open(os.path.join(side_path, \"joint_angles.json\"), \"r\") as f:\n",
    "            side_view = json.load(f)\n",
    "        with open(os.path.join(top_path, \"joint_angles.json\"), \"r\") as f:\n",
    "            top_view = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è Missing joint_angles.json for {player_name}. Skipping.\")\n",
    "        return save_path\n",
    "\n",
    "    # Frame intersection and sorting\n",
    "    common_frames = set(front_view.keys()) & set(side_view.keys()) & set(top_view.keys())\n",
    "    sorted_frames = sorted(common_frames, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "\n",
    "    X, y, t, metadata = [], [], [], []\n",
    "    skipped_count = 0\n",
    "\n",
    "    for i in range(len(sorted_frames) - seq_len + 1):\n",
    "        sequence, skip_sequence = [], False\n",
    "        angle_sums = {k: 0 for k in TARGETS}\n",
    "\n",
    "        current_frames = sorted_frames[i:i + seq_len]\n",
    "\n",
    "        for frame in current_frames:\n",
    "            try:\n",
    "                features = [\n",
    "                    front_view[frame][\"right_elbow_angle\"],\n",
    "                    front_view[frame][\"left_elbow_angle\"],\n",
    "                    side_view[frame][\"knee_angle\"],\n",
    "                    side_view[frame][\"hip_angle\"],\n",
    "                    side_view[frame][\"ankle_angle\"],\n",
    "                    side_view[frame][\"back_angle\"],\n",
    "                    side_view[frame][\"elbow_angle\"],\n",
    "                    top_view[frame][\"shoulder_symmetry\"],\n",
    "                    top_view[frame][\"hip_symmetry\"],\n",
    "                    top_view[frame][\"wrist_alignment\"]\n",
    "                ]\n",
    "\n",
    "                if any(not (0 <= val <= 180) for val in features):\n",
    "                    raise ValueError(\"Angle out of 0‚Äì180 range\")\n",
    "\n",
    "                # Summing for feedback generation\n",
    "                angle_sums[\"back_angle\"] += features[5]\n",
    "                angle_sums[\"knee_angle\"] += features[2]\n",
    "                angle_sums[\"shoulder_symmetry\"] += features[7]\n",
    "                angle_sums[\"hip_symmetry\"] += features[8]\n",
    "                angle_sums[\"elbow_angle\"] += features[6]\n",
    "\n",
    "                sequence.append(features)\n",
    "\n",
    "            except (KeyError, ValueError) as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping sequence {i} due to error in frame {frame}: {e}\")\n",
    "                skip_sequence = True\n",
    "                break\n",
    "\n",
    "        if skip_sequence:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        avg_features = {k: v / seq_len for k, v in angle_sums.items()}\n",
    "        feedback = generate_feedback_continuous(avg_features)\n",
    "        feedback_vector = [feedback[cat] for cat in FEEDBACK_CATEGORIES]\n",
    "\n",
    "        X.append(sequence)\n",
    "        y.append(feedback_vector)\n",
    "        t.append(technique)\n",
    "\n",
    "        metadata.append({\n",
    "            \"video_path\": folder_path,\n",
    "            \"frames_used\": current_frames,\n",
    "            \"avg_features\": avg_features,\n",
    "            \"feedback\": feedback,\n",
    "            \"sequence_index\": i\n",
    "        })\n",
    "\n",
    "    # Final dataset conversion\n",
    "    X, y, t = np.array(X), np.array(y), np.array(t)\n",
    "\n",
    "    np.savez(save_path, X=X, y=y, t=t)\n",
    "    print(f\"‚úÖ Dataset saved: {save_path}\")\n",
    "    print(f\"üìä Sequences: {len(X)}, Skipped: {skipped_count}\")\n",
    "\n",
    "    with open(metadata_log_path, \"w\") as mf:\n",
    "        json.dump(metadata, mf, indent=2)\n",
    "        print(f\"üìù Metadata log saved: {metadata_log_path}\")\n",
    "\n",
    "    processed_players.add(player_name)\n",
    "    return save_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b878c",
   "metadata": {},
   "source": [
    "## Step 5 - Model Architecture Designing and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ab2af",
   "metadata": {},
   "source": [
    "#### Sklearn Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fc2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d365d4c8",
   "metadata": {},
   "source": [
    "#### Tenserflow install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fafd3d6",
   "metadata": {},
   "source": [
    "#### GRU Regression Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0390ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === Load the regression dataset from .npz ===\n",
    "data = np.load(\"regression_dataset_snatch.npz\")\n",
    "X = data[\"X\"]  # shape: (samples, seq_len, features)\n",
    "y = data[\"y\"]  # shape: (samples, feedback categories)\n",
    "t = data[\"t\"]  # shape: (samples,)\n",
    "\n",
    "# === Train-validation split ===\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Define GRU regression model ===\n",
    "def build_gru_regression_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(64, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(64, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='linear'))  # Linear output for regression\n",
    "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "# === Build model ===\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (seq_len, features)\n",
    "model = build_gru_regression_model(input_shape)\n",
    "\n",
    "# === Train model ===\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# === Save model ===\n",
    "model.save(\"gru_posture_model_snatch_regression.h5\")\n",
    "\n",
    "# === Print training history ===\n",
    "print(\"‚úÖ Training complete!\")\n",
    "print(history.history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f749a590",
   "metadata": {},
   "source": [
    "## Automatic Pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "190de439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# ======================= #\n",
    "# ==== CONFIGURATION ==== #\n",
    "# ======================= #\n",
    "\n",
    "# Hardcoded output paths for consistency\n",
    "FRAMES_ROOT = \"Output/frames\"\n",
    "KEYPOINTS_ROOT = \"Output/keypoints\"\n",
    "ANNOTATIONS_ROOT = \"Output/annotations\"\n",
    "LOG_FILE = \"Output/processing_log.txt\"\n",
    "CENTRAL_INDEX_FILE = \"Output/central_index.csv\"\n",
    "\n",
    "\n",
    "# Features to extract from the video\n",
    "\n",
    "ANGLES_ROOT = \"Output/angles\"\n",
    "\n",
    "# Create all required output folders at startup\n",
    "for path in [FRAMES_ROOT, KEYPOINTS_ROOT, ANNOTATIONS_ROOT, ANGLES_ROOT, os.path.dirname(LOG_FILE)]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# =============================== #\n",
    "# ==== UTILITY HELPER FUNCS ==== #\n",
    "# =============================== #\n",
    "\n",
    "# This function retrieves all video files from the specified root folder and its subdirectories.\n",
    "## It supports multiple file extensions (e.g., .mp4, .mov) and returns a list of full paths.\n",
    "def get_all_video_paths(root_folder, extensions=(\".mp4\", \".mov\")):\n",
    "    video_paths = []\n",
    "    for dirpath, _, filenames in os.walk(root_folder):\n",
    "        for file in filenames:\n",
    "            if file.lower().endswith(extensions):\n",
    "                video_paths.append(os.path.join(dirpath, file))\n",
    "    return video_paths\n",
    "\n",
    "\n",
    "# This function generates a relative path for a video file based on the root input folder.\n",
    "def get_relative_path(video_path, root_folder):\n",
    "    return os.path.relpath(video_path, root_folder)\n",
    "\n",
    "# This function builds output paths for frames, keypoints, annotations, and metadata based on the video path.\n",
    "# It uses the relative path to create a consistent structure under the defined output folders.\n",
    "def build_output_paths(video_path, root_input_folder):\n",
    "    relative_path = get_relative_path(video_path, root_input_folder)\n",
    "    base = os.path.splitext(relative_path)[0]\n",
    "    return {\n",
    "        \"frames\": os.path.join(FRAMES_ROOT, base),\n",
    "        \"keypoints\": os.path.join(KEYPOINTS_ROOT, base),\n",
    "        \"annotations\": os.path.join(ANNOTATIONS_ROOT, base),\n",
    "        \"metadata\": os.path.join(KEYPOINTS_ROOT, base, \"metadata.json\"),\n",
    "        \"angles\": os.path.join(ANGLES_ROOT, base)  #features\n",
    "    }\n",
    "\n",
    "# This function checks if a video has already been processed by looking for the metadata file.\n",
    "# If the metadata file exists, it indicates that the video has been processed.\n",
    "def has_been_processed(paths):\n",
    "    return os.path.exists(paths[\"metadata\"])\n",
    "\n",
    "# This function logs messages to a log file and prints them to the console.\n",
    "# It includes a timestamp for each message.\n",
    "def log_message(message):\n",
    "    timestamp = f\"{datetime.now()} - {message}\"\n",
    "    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as log_file:\n",
    "        log_file.write(timestamp + \"\\n\")\n",
    "    print(timestamp)\n",
    "\n",
    "# This function saves metadata about the processed video to a JSON file.\n",
    "# It includes the video path, processing time, and paths to frames and keypoints.\n",
    "def save_metadata(metadata_path, data):\n",
    "    os.makedirs(os.path.dirname(metadata_path), exist_ok=True)\n",
    "    with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# This function updates a central index CSV file with metadata about processed videos.\n",
    "# It appends a new row with the video path, frame folder, keypoints folder, annotation folder, and processing time.\n",
    "def update_central_index(index_file, row):\n",
    "    os.makedirs(os.path.dirname(index_file), exist_ok=True)\n",
    "    file_exists = os.path.exists(index_file)\n",
    "    with open(index_file, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=row.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "\n",
    "# ===================================== #\n",
    "# ==== MAIN VIDEO PROCESSOR FUNC  ==== #\n",
    "# ===================================== #\n",
    "\n",
    "def process_all_videos(root_input_folder):\n",
    "    \"\"\"\n",
    "    Main function to process all videos in the specified root input folder.\n",
    "    It extracts frames, detects keypoints, and saves metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all video paths from the root input folder\n",
    "    videos = get_all_video_paths(root_input_folder)\n",
    "    # If no videos found, log and exit\n",
    "    log_message(f\"üé¨ Found {len(videos)} videos under '{root_input_folder}'\")\n",
    "\n",
    "    # If no videos found, exit early\n",
    "    for video_path in videos:\n",
    "        paths = build_output_paths(video_path, root_input_folder)\n",
    "        if has_been_processed(paths):\n",
    "            log_message(f\"‚è≠Ô∏è Skipping already processed: {video_path}\")\n",
    "            continue\n",
    "\n",
    "        # Create required output folders\n",
    "        for key in [\"frames\", \"keypoints\", \"annotations\", \"angles\"]:\n",
    "            os.makedirs(paths[key], exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            # ====== MAIN PIPELINE STEPS ======\n",
    "            # ====== Adjust these steps as needed ======\n",
    "            \n",
    "            # STEP 1: Extract frames from the video\n",
    "            extract_frames(video_path, paths[\"frames\"])  # ok no problem\n",
    "\n",
    "            # STEP 2: Detect keypoints in the frames\n",
    "            detect_keypoints(paths[\"frames\"], paths[\"annotations\"])  # ok not problem\n",
    "            \n",
    "            # STEP 3: Extract 2D keypoints from the annotated frames\n",
    "            extract_2d_keypoints(paths[\"frames\"], os.path.join(paths[\"keypoints\"], \"keypoints.json\"))  # ok\n",
    "            \n",
    "            # STEP 4: Extract joint angles from keypoints\n",
    "            extract_joint_angles(os.path.join(paths[\"keypoints\"], \"keypoints.json\"), os.path.join(paths[\"angles\"], \"joint_angles.json\"))  # STEP 4\n",
    "\n",
    "            \n",
    "            # STEP 5: Create regression dataset\n",
    "            dataset_csv_path = \"Output/dataset/angles_dataset.csv\"\n",
    "            create_regression_dataset(os.path.join(paths[\"angles\"], \"joint_angles.json\"), dataset_csv_path)\n",
    "\n",
    "\n",
    "            # ====== End of processing steps ======\n",
    "            \n",
    "            # ====== Save metadata ======\n",
    "            metadata = {\n",
    "                \"video\": video_path,\n",
    "                \"processed_at\": str(datetime.now()),\n",
    "                \"frames_folder\": paths[\"frames\"],\n",
    "                \"keypoints_json\": os.path.join(paths[\"keypoints\"], \"keypoints.json\")\n",
    "            }\n",
    "            save_metadata(paths[\"metadata\"], metadata)\n",
    "\n",
    "            # ====== Update tracking CSV ======\n",
    "            update_central_index(CENTRAL_INDEX_FILE, {\n",
    "                \"video_path\": video_path,\n",
    "                \"frames_folder\": paths[\"frames\"],\n",
    "                \"keypoints_folder\": paths[\"keypoints\"],\n",
    "                \"annotation_folder\": paths[\"annotations\"],\n",
    "                \"processed_at\": metadata[\"processed_at\"]\n",
    "            })\n",
    "\n",
    "            log_message(f\"‚úÖ Processed: {video_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log_message(f\"‚ùå Error processing {video_path}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb155eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-27 17:50:03.553157 - üé¨ Found 3 videos under 'Source'\n",
      "2025-05-27 17:50:03.554832 - ‚è≠Ô∏è Skipping already processed: Source\\Player 2\\CP\\Player02_CP_Set_01\\Player02_CP_60Kg_front_view.MOV\n",
      "2025-05-27 17:50:03.557877 - ‚è≠Ô∏è Skipping already processed: Source\\Player 2\\CP\\Player02_CP_Set_01\\Player02_CP_60Kg_side_view.MOV\n",
      "2025-05-27 17:50:03.557877 - ‚è≠Ô∏è Skipping already processed: Source\\Player 2\\CP\\Player02_CP_Set_01\\Player02_CP_60Kg_top_view.mp4\n"
     ]
    }
   ],
   "source": [
    "process_all_videos(\"Source\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
